# gbdt理解
机器学习都得有**模型**，以及**损失函数**。
## 模型
![](/home/martin/Documents/picture_md/1.png)

确定分割值确定了整个模型的结构，实际数据很多都是定量数据，这就涉及取阈值。这个其实是很重要的工作，但是很多文章都是介绍下面这个，也就是确定当前确定结构模型的参数，就是W。
## 损失函数
![](/home/martin/Documents/picture_md/2.png)

L函数中只有一个变量就是各期的预测值，g<sub>i</sub>就是L函数的一阶导数在上期的取值。f<sub>i</sub>就是各个样本对应的w。

当前模型惩罚系数：

![](http://www.52cs.org/wp-content/uploads/2015/04/10.png)

把按照样本求和变成分成各个叶子节点然后再求和，原目标函数变成：

![](http://www.52cs.org/wp-content/uploads/2015/04/a.png)

模型结构确定后，各个节点的g，h都确定了，T也定了。$\lambda\gamma$也都定了，变的只有w。

![](/home/martin/Documents/picture_md/Firefox_Screenshot_2017-09-18T16-22-15.673Z.png)
模型只要结构确定了，参数w等于确定了。并且模型得分也知道了。sum部分可以看做每个叶子节点带来的信息增益。
## 确定模型(单个树）

![](http://www.52cs.org/wp-content/uploads/2015/04/14.png)

左边划分阈值经历的过程如下：

![](http://www.52cs.org/wp-content/uploads/2015/04/image1.png)

选取得分最高时的阈值，当然这个的前提是选特征。选取不同的特征都有其最优的阈值，选择得分最高的那个特征。
这里还有一个没有解决的问题就是讲道理最优树只有一个，从最开始一个叶子节点也就是没有划分，到你自己设置的深度，对于确定的样本，这个模型是确定的。那么怎么构造出不同的树，然后ensemble。
## 生成多棵树 
按照上面的方法训练出一棵树，然后每个样本都有个残差。这时候我们不再继续在这棵树下进行拟合。把y值替换为新的这些残差，然后重新按照上面的方法进行拟合。这么做是有意义的，把残差提取出来，完全重新拟合一棵新树，这样忽略掉原先各个样本在哪个节点。

# xgboost和gbdt区别
(1)传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）

(2)传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。

(3)xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance trade-off角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。

(4)Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）

(5)列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。

(6)对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。

(7)xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。

(8)可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。

作者：任性的小飞熊
链接：http://www.jianshu.com/p/af1fbcd6058d
來源：简书
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
